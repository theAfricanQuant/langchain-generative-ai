{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# parent_dir = Path(os.getcwd()).parent\n",
    "# sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from config import set_environment\n",
    "\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "summary = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='print(\"Hello world\")' response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 15, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9fe7714a-1027-4910-894c-f788ea7acf99-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the chat model:\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0613\")\n",
    "# Generate a conversation:\n",
    "response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As your helpful assistant, I can provide you with more information on LangChain and LangGraph, as well as assist you in using these frameworks to build powerful language-based applications. Just let me know how I can help!\n",
      "\n",
      "This text explains how the assistant can provide information and assistance in using LangChain and LangGraph to create language-based applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "text = \"\"\"\n",
    "LangChain is a framework designed to build applications using language models, enabling users \n",
    "to integrate large language models (LLMs) with external data sources, APIs, or custom chains of \n",
    "logic. It provides tools for prompting, managing and orchestrating interactions with LLMs, and is \n",
    "often used for tasks like agent-based systems, question-answering, and data pipelines.\n",
    "\n",
    "A LangGraph is a framework or structure used to organize and visualize language models (LMs) \n",
    "and their interactions with various data sources, processes, or other models in a graph-like \n",
    "format. It typically represents different components (e.g., models, external APIs, databases) \n",
    "as nodes, and the relationships or data flow between them as edges. LangGraph helps in managing \n",
    "complex pipelines where multiple LLMs, tools, and data interact, enabling easier debugging, scalability, \n",
    "and management of tasks such as data retrieval, transformation, and model inference. It's useful for applications\n",
    "that require complex orchestration, such as conversational AI, agent-based systems, or workflows involving multiple \n",
    "LLMs working together to achieve specific objectives.\n",
    "\n",
    "\"\"\"\n",
    "chat_output = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You're a helpful assistant\"),\n",
    "        HumanMessage(content=text),\n",
    "    ]\n",
    ")\n",
    "print(chat_output)\n",
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "{text}\n",
    "\"\"\"\n",
    "llm = OpenAI()\n",
    "summary = llm.invoke(prompt.format(text=chat_output))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
