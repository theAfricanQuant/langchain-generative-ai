{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# parent_dir = Path(os.getcwd()).parent\n",
    "# sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from config import set_environment\n",
    "\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "summary = llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='print(\"Hello world\")' response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 15, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9fe7714a-1027-4910-894c-f788ea7acf99-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the chat model:\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-0613\")\n",
    "# Generate a conversation:\n",
    "response = llm.invoke([HumanMessage(content='Say \"Hello world\" in Python.')])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As your helpful assistant, I can provide you with more information on LangChain and LangGraph, as well as assist you in using these frameworks to build powerful language-based applications. Just let me know how I can help!\n",
      "\n",
      "This text explains how the assistant can provide information and assistance in using LangChain and LangGraph to create language-based applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "text = \"\"\"\n",
    "LangChain is a framework designed to build applications using language models, enabling users \n",
    "to integrate large language models (LLMs) with external data sources, APIs, or custom chains of \n",
    "logic. It provides tools for prompting, managing and orchestrating interactions with LLMs, and is \n",
    "often used for tasks like agent-based systems, question-answering, and data pipelines.\n",
    "\n",
    "A LangGraph is a framework or structure used to organize and visualize language models (LMs) \n",
    "and their interactions with various data sources, processes, or other models in a graph-like \n",
    "format. It typically represents different components (e.g., models, external APIs, databases) \n",
    "as nodes, and the relationships or data flow between them as edges. LangGraph helps in managing \n",
    "complex pipelines where multiple LLMs, tools, and data interact, enabling easier debugging, scalability, \n",
    "and management of tasks such as data retrieval, transformation, and model inference. It's useful for applications\n",
    "that require complex orchestration, such as conversational AI, agent-based systems, or workflows involving multiple \n",
    "LLMs working together to achieve specific objectives.\n",
    "\n",
    "\"\"\"\n",
    "chat_output = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You're a helpful assistant\"),\n",
    "        HumanMessage(content=text),\n",
    "    ]\n",
    ")\n",
    "print(chat_output)\n",
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "{text}\n",
    "\"\"\"\n",
    "llm = OpenAI()\n",
    "summary = llm.invoke(prompt.format(text=chat_output))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ricky\\.cache\\huggingface\\token\n",
      "Login successful\n",
      " The FIFA World Cup is an international football competition. It is held every four years. The year 1994 is indeed a year that fits the pattern of the World Cup. Now, let's find out who won it. The 1994 FIFA World Cup was held in the United States. The final match was played on July 17, 1994, between Brazil and Italy. Brazil won the match 0-0 (after extra time) and 3-2 in a penalty shootout. Therefore, Brazil won the FIFA World Cup in the year 1994.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    ")\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "summary = runnable.invoke({\"question\": question})\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
