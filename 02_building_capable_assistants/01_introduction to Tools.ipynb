{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build capable assistants with LLMs, it's crucial to enhance their abilities with tools, specific agent architectures, and methods for extracting structured information and fact-checking. The following points summarize how to achieve this:\n",
    "\n",
    "*   **Tool Integration**:\n",
    "    *   LLMs can be augmented by connecting them to external data and services.\n",
    "    *   Tools allow LLMs to interact with the real world, access real-time data, and perform tasks, thus overcoming their limitations with domain-specific or up-to-date knowledge.\n",
    "    *   Examples include tools for web searches, database queries, email automation, or even handling phone calls.\n",
    "    *   LangChain provides a platform to create these tools which can be used by agents, chains, or LLMs to interact with the world.\n",
    "    *   Tools encapsulate a name, a function to execute, a description for the LLM, an optional schema for parameters, and a flag to return directly to the user.\n",
    "    *   Tools can be built-in, like WikipediaQueryRun, or custom-defined using decorators, subclassing, or dataclasses.\n",
    "\n",
    "*   **Custom Tool Definition**:\n",
    "    *   The `@tool` decorator can be used to define a tool, using the function name as the tool name and the docstring as the description.\n",
    "    *   Subclassing `BaseTool` allows for more control, enabling customization of behavior, complex logic, asynchronous operations, and error handling.\n",
    "    *   `StructuredTool` offers a balance between the complexity of subclassing and the simplicity of the decorator.\n",
    "    *   Error handling in tools is important to allow agents to continue executing even when a tool encounters an error.\n",
    "\n",
    "*   **Agent Architectures:**\n",
    "    *   Agents combine LLMs with tools to perform tasks, utilizing specific reasoning strategies.\n",
    "    *   **Action agents** reason iteratively based on observations after each action.\n",
    "    *  **Plan-and-execute agents** plan completely upfront before taking any action, then gather evidence to execute the plan.\n",
    "    *   In plan-and-execute architectures, a Planner LLM creates a plan, then an agent gathers evidence with tools, and finally, a Solver LLM generates the output.\n",
    "\n",
    "*   **Building a Research Assistant**:\n",
    "    *   A basic research assistant can be constructed using LangChain by combining an LLM with external tools to gather information.\n",
    "    *   Tools like DuckDuckGo, Wolfram Alpha, arXiv, and Wikipedia can be used to enhance the agent’s capabilities.\n",
    "    *   The Streamlit framework provides an easy-to-use platform for wrapping the agent in a web application.\n",
    "    *   Streamlit enables creating interactive web applications, integrating the agent's capabilities, and allows for real-time responses.\n",
    "\n",
    "*   **Extracting Structured Information**:\n",
    "    *   LangChain facilitates extracting structured information from documents by combining LLMs with schema definitions.\n",
    "    *   Pydantic can be used to create custom data structures (models) for defining the desired output format.\n",
    "    *   LangChain's output parsers transform LLM outputs into structured formats, such as JSON or Pydantic models.\n",
    "\n",
    "*   **Mitigating Hallucinations**:\n",
    "    *   Automatic fact-checking is crucial for verifying claims made by LLMs against external sources.\n",
    "    *   Fact-checking includes claim detection, evidence retrieval, and verdict prediction.\n",
    "    *  The `LLMCheckerChain` in LangChain can be used to make a model check its own assumptions.\n",
    "\n",
    "By integrating these elements, more capable, reliable, and trustworthy LLM-powered assistants can be developed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine how to use a tool in LangChain. We will use the `tool.name` toexamine the to examine the tool's name, `tool.description` to get the tool's description, and `tool.args` to get the tool's arguments, `tool.return_direct` to get the tool's return value and `tool.run()` to execute the tool.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: wikipedia\n",
      "Description: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Arguments: {'query': {'title': 'Query', 'type': 'string'}}\n",
      "Return Direct: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tool Name: {tool.name}\\n\"\n",
    "      f\"Description: {tool.description}\\n\"\n",
    "      f\"Arguments: {tool.args}\\n\"\n",
    "      f\"Return Direct: {tool.return_direct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run({\"query\": \"langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom tools\n",
    "To define custom tools in LangChain, you can follow approaches such as these:\n",
    "\n",
    "* @tool decorator\n",
    "* Subclassing BaseTool\n",
    "* StructuredTool dataclass\n",
    "\n",
    "### **1.    The `@tool` decorator**\n",
    "\n",
    "In Python, we can define a tool using the decorator syntax, which simplifies the process of adding functionality to a function. By default, when we apply a decorator, it automatically assigns the function's name as the tool's name and the function's docstring as the tool's description. This means that the function’s docstring, which explains what the function does, is automatically used as the tool's description in the context of the decorator. Therefore, it’s important to always include a meaningful docstring, as it provides the necessary description of the tool's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "# Ask our question\n",
    "search(\"What's the best application framework for LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also customize the tool name and JSON args by passing them into the tool decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "\n",
    "@tool(\"search-tool\", args_schema=SearchInput, return_direct=True)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "search(\"How do we implement LLM apps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2.    Subclassing BaseTool**\n",
    "\n",
    "Subclassing `BaseTool` provides full control over tool definition but requires more effort. It’s ideal when you need to:\n",
    "\n",
    "- Customize tool behavior\n",
    "- Implement complex logic\n",
    "- Handle async operations\n",
    "- Manage errors or logging\n",
    "\n",
    "This approach allows you to define:\n",
    "\n",
    "- Tool metadata (name, description)\n",
    "- Input schema\n",
    "- Sync/async execution methods\n",
    "- Custom error handling and callbacks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Type\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "    args_schema: Type[BaseModel] = SearchInput\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return \"LangChain\"\n",
    "\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "\n",
    "search = CustomSearchTool()\n",
    "search(\"What's the most popular tool for writing LLM apps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define a custom input schema (SearchInput)**\n",
    "\n",
    "```python\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "```\n",
    "\n",
    "Here, a class `SearchInput` is defined to serve as the input schema for the tool. It inherits from `BaseModel` and uses `Field` to specify that the input should include a `query` field. This is the user input that the tool will process, and the description provides clarity on its expected content.\n",
    "\n",
    "**The tool’s name and description are class attributes**\n",
    "\n",
    "```python\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "```\n",
    "\n",
    "The `CustomSearchTool` class inherits from `BaseTool` and sets two key class attributes: `name` and `description`. These attributes provide essential metadata for the tool, allowing it to be identified by name (\"custom_search\") and providing a brief overview of its purpose—answering questions about current events.\n",
    "\n",
    "**The _run method implements the synchronous tool execution**\n",
    "\n",
    "```python\n",
    "def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "    \"\"\"Use the tool.\"\"\"\n",
    "    return \"LangChain\"\n",
    "```\n",
    "\n",
    "The `_run` method defines how the tool will behave when invoked synchronously. It accepts a `query` argument and an optional `run_manager`. In this case, the method simply returns the string `\"LangChain\"`. The optional `run_manager` enables advanced control over the execution, though it’s not used here.\n",
    "\n",
    "**The _arun method is a placeholder for asynchronous execution**\n",
    "\n",
    "```python\n",
    "async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "    \"\"\"Use the tool asynchronously.\"\"\"\n",
    "    raise NotImplementedError(\"custom_search does not support async\")\n",
    "```\n",
    "\n",
    "The `_arun` method is defined as an asynchronous counterpart to `_run`. However, it raises a `NotImplementedError`, indicating that asynchronous execution is not supported for this tool. The method still accepts the `query` and `run_manager` arguments, but it serves only as a placeholder for future potential asynchronous support.\n",
    "\n",
    "**Both methods include optional callback managers for advanced control**\n",
    "\n",
    "Both `_run` and `_arun` methods include optional `run_manager` parameters. These are instances of callback managers (`CallbackManagerForToolRun` and `AsyncCallbackManagerForToolRun`), which are used to control execution flow, handle callbacks, and manage advanced features like logging or error handling. The inclusion of these parameters makes the tool extensible and adaptable to complex workflows, even if they are not actively used in this example.\n",
    "\n",
    "**StructuredTool dataclass**\n",
    "\n",
    "StructuredTool offers a convenient way to define tools for Langchain workflows. It provides a balance between inheriting from the base BaseTool class (more complex) and simply using a decorator (less functionality).\n",
    "\n",
    "---\n",
    "\n",
    "Now, let us create a tool named \"Search\" using `StructuredTool.from_function` and see how it works.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "\n",
    "def search_function(query: str):\n",
    "    return \"LangChain\"\n",
    "\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_function,\n",
    "    name=\"Search\",\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    ")\n",
    "search(\"Which framework has hundreds of integrations to use with LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a simple function `search_function` that always returns `\"LangChain\"`. It then creates a `StructuredTool` object called `search`, assigning it the `function`, `name`, and `description`. Lastly, it calls the `run` method on the `search` tool with a query string, illustrating the tool’s usage.\n",
    "\n",
    "`StructuredTool` allows you to define a custom input schema using a `BaseModel` subclass. This provides better type checking and documentation for your tool’s inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"first number\")\n",
    "    b: int = Field(description=\"second number\")\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "calculator = StructuredTool.from_function(\n",
    "    func=multiply,\n",
    "    name=\"Calculator\",\n",
    "    description=\"multiply numbers\",\n",
    "    args_schema=CalculatorInput,\n",
    "    return_direct=True,\n",
    ") \n",
    "\n",
    "calculator.run(dict(a=1_000_000_000, b=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, a `CalculatorInput` class is defined, specifying the input format for the tool. It uses `BaseModel` from the `pydantic` library to define two fields: `a` and `b`, both of which are integers. The `Field` function provides descriptions for these inputs, explaining that `a` is the first number and `b` is the second number. This class enforces type checking, ensuring that the tool only accepts valid input formats.\n",
    "\n",
    "The `multiply` function is then defined, which simply multiplies the two input numbers `a` and `b`. The function returns the product of these two integers, and it is annotated to return an integer (`int`).\n",
    "\n",
    "Next, a `StructuredTool` object named `calculator` is created using the `StructuredTool.from_function` method. This method binds the `multiply` function to the tool, setting the `name` to `\"Calculator\"` and providing a `description` that states it is used to multiply numbers. The `args_schema` is set to `CalculatorInput`, meaning the tool will expect input in the format defined by the `CalculatorInput` class.\n",
    "\n",
    "The `return_direct=True` flag ensures that the result from the `multiply` function is returned directly, without any additional processing or transformations.\n",
    "\n",
    "Finally, the `run` method of the `calculator` tool is called, passing a dictionary with the values `a=1_000_000_000` and `b=2`. The tool then executes the `multiply` function with these inputs and returns the product, which is `2_000_000_000`.\n",
    "\n",
    "\n",
    "### **Error Handling in Tools**\n",
    "```python\n",
    "from langchain_core.tools import ToolException\n",
    "\n",
    "def search_tool(s: str):\n",
    "    raise ToolException(\"The search tool is not available.\")\n",
    "\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_tool,\n",
    "    name=\"Search_tool\",\n",
    "    description=\"A bad tool\",\n",
    "    handle_tool_error=True,\n",
    ")\n",
    "\n",
    "search(\"Search the internet and compress everything into a paragraph!\")\n",
    "```\n",
    "\n",
    "This code snippet introduces error handling within a LangChain tool by leveraging the `ToolException` class. The `search_tool` function is designed to intentionally raise an error, simulating a failure in the tool. The error message `\"The search tool is not available.\"` clearly indicates that the tool has encountered an issue.\n",
    "\n",
    "To ensure that the agent continues execution even when this error occurs, the `handle_tool_error` parameter is set to `True` when creating the `StructuredTool` object. This instructs LangChain to handle the error without halting the execution of the agent.\n",
    "\n",
    "The execution of the tool with the `search` function causes the `ToolException` to be raised, and without proper error handling, this would stop the agent. However, by setting `handle_tool_error=True`, the tool informs LangChain that it should manage this error internally and allow the agent to proceed. The specific error is raised but won't disrupt the tool's operation unless explicitly handled.\n",
    "\n",
    "Furthermore, LangChain offers the ability to define a custom error-handling strategy by passing a function to `handle_tool_error` instead of a simple `True`. This function should accept a `ToolException` and return a `str`, which allows for more refined control over the error response, possibly transforming the error into a more user-friendly message or logging it for later analysis.\n",
    "\n",
    "In this case, the tool raises an error, but the agent does not stop; the tool simply informs the user that the search functionality is unavailable. This is part of LangChain's flexibility, which allows for efficient error management while maintaining the flow of execution—particularly useful when building more complex applications like a research assistant, where various tools might fail without bringing down the entire system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
