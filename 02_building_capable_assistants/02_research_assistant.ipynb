{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains how to build a basic research assistant using LangChain by combining an LLM with external tools to gather information and respond to user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from config import set_environment\n",
    "\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import (\n",
    "    AgentExecutor, load_tools, create_react_agent\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def load_agent() -> AgentExecutor:\n",
    "    llm = ChatOpenAI(temperature=0, streaming=True)\n",
    "    tools = load_tools(\n",
    "        tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
    "        llm=llm\n",
    "    )\n",
    "    return AgentExecutor(\n",
    "        agent=create_react_agent(llm=llm, tools=tools), tools=tools\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_agent()` function creates a LangChain agent by initializing a `ChatOpenAI` LLM with streaming enabled, enhancing user interaction. The `load_tools()` function loads specified tools like DuckDuckGoSearch, arXiv, and Wikipedia, which the agent uses to gather information. The agent combines the LLM with these tools, creating a powerful system for answering queries. The Zero-Shot agent (or ReAct agent) is a versatile agent designed for general-purpose action, capable of dynamically selecting and using the appropriate tools to respond to a wide range of inputs.\n",
    "\n",
    "The `streaming` parameter in the `ChatOpenAI` constructor is set to `True` to improve the user experience. This enables the response text to be updated incrementally as it is generated, rather than waiting for the entire response to be completed. Currently, only the `OpenAI`, `ChatOpenAI`, and `ChatAnthropic` implementations support this streaming feature.\n",
    "\n",
    "## Visial Interface\n",
    "\n",
    "After building an agent with LangChain, the next logical step is to deploy it within an easy-to-use application. Streamlit serves as an excellent framework for this purpose. It is an open-source platform designed specifically for machine learning workflows, making it straightforward to package and present our agent as an interactive web application. With Streamlit, we can quickly create a user-friendly interface for our agent that allows seamless interaction.\n",
    "\n",
    "### Alternative Frameworks to Streamlit:\n",
    "Though Streamlit is a great choice, there are several other platforms worth considering, each with its strengths:\n",
    "- **Gradio**: Similar to Streamlit but optimized for machine learning demos.\n",
    "- **Dash**: Offers more customization options, though it's more complex to use.\n",
    "- **Panel**: Focused on building flexible dashboards and web applications.\n",
    "- **Anvil**: Enables full-stack web app development using Python.\n",
    "- **Voilà**: Converts Jupyter Notebooks into interactive web applications.\n",
    "- **Taipy**: Known for its performance and scalability in high-demand applications.\n",
    "\n",
    "For this example, we’ll stick with **Streamlit**, given its ease of use, broad adoption in the ML community, and simplicity for quickly deploying models and agents as web applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.callbacks.streamlit import (\n",
    "    StreamlitCallbackHandler,\n",
    ")\n",
    "chain = load_agent()\n",
    "st_callback = StreamlitCallbackHandler(st.container())\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st_callback = StreamlitCallbackHandler(st.container())\n",
    "        response = chain.run(prompt, callbacks=[st_callback])\n",
    "        st.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Imports**  \n",
    "```python\n",
    "import streamlit as st\n",
    "from langchain_community.callbacks.streamlit import StreamlitCallbackHandler\n",
    "```\n",
    "- `streamlit as st`: Imports Streamlit, which is used to build the web interface.\n",
    "- `StreamlitCallbackHandler`: This handler is imported from LangChain’s community tools to enable Streamlit-specific callbacks when using LangChain.\n",
    "\n",
    "**Load Agent**  \n",
    "```python\n",
    "chain = load_agent()\n",
    "```\n",
    "- `load_agent()`: This function presumably loads an agent or a chain of tools that have been preconfigured, likely an LLM (like OpenAI) agent or other tools in LangChain. This chain will process the input and generate responses.\n",
    "\n",
    "**Streamlit Callback Handler**  \n",
    "```python\n",
    "st_callback = StreamlitCallbackHandler(st.container())\n",
    "```\n",
    "- `StreamlitCallbackHandler(st.container())`: The callback handler is initialized with a Streamlit container, allowing it to send updates to Streamlit's interface during the agent's execution. The `st.container()` function is used to organize the layout of elements in the app.\n",
    "\n",
    "**Handle User Input**  \n",
    "```python\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "```\n",
    "- `if prompt := st.chat_input()`: This line checks if the user has entered a message in the chat input field. If there is input (`prompt`), the code executes the block inside the `if` statement.\n",
    "- `st.chat_message(\"user\").write(prompt)`: This displays the user's input in the chat window with the label \"user.\"\n",
    "\n",
    "**Generate Assistant’s Response**  \n",
    "```python\n",
    "with st.chat_message(\"assistant\"):\n",
    "    st_callback = StreamlitCallbackHandler(st.container())\n",
    "    response = chain.run(prompt, callbacks=[st_callback])\n",
    "    st.write(response)\n",
    "```\n",
    "- `with st.chat_message(\"assistant\")`: This block specifies that the following content will be displayed as the assistant's message in the chat.\n",
    "- `st_callback = StreamlitCallbackHandler(st.container())`: A new callback handler is initialized for each interaction to manage streaming and updates for the assistant’s response.\n",
    "- `response = chain.run(prompt, callbacks=[st_callback])`: This sends the user’s input (`prompt`) to the `chain` (the LangChain agent) for processing. The `callbacks` parameter allows the `StreamlitCallbackHandler` to update the Streamlit app with the response.\n",
    "- `st.write(response)`: This displays the assistant's generated response in the Streamlit app.\n",
    "\n",
    "We will create a `app.py` file and can start the app locally from the terminal like this:\n",
    "\n",
    "`PYTHONPATH=. streamlit run app.py`\n",
    "\n",
    "**in a nutshell**  \n",
    "This code creates a simple chat interface in Streamlit where users can enter a prompt. The prompt is processed by a LangChain agent (`chain`), and the response is displayed in the chat window. The `StreamlitCallbackHandler` ensures that the chat interface is updated smoothly as the agent processes the prompt, providing an interactive user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
